# 多媒体技术课堂提问

## 特征脸(EigenFace)与PCA的关系
 特征脸算法是基于主成分分析(PCA)的人脸识别方法。特征子脸技术的基本思想是：从统计的观点，寻找人脸图像分布的基本元素，即人脸图像样本集协方差矩阵的特征向量，以此近似地表征人脸图像。**(总)**

特征脸算法的基本步骤如下：

1. **构建样本集合**：获取包含有M张人脸图像的集合S，每张人脸图片的大小缩放到统一的尺寸，得到图像矩阵集合
   $$
   \begin{equation*}
   S=\left\{\Gamma_{1}, \Gamma_{2}, \Gamma_{3}, \ldots \ldots \ldots, \Gamma_{M}\right\}
   \end{equation*}
   $$

2. **求得图像矩阵均值，也就是平均脸**：
   $$
   \begin{equation*}
   \displaystyle mean =\frac{1}{M} \sum_{n=1}^{M} \Gamma_{n}
   \end{equation*}
   $$

3. **图像矩阵零均值化**
   $$
   \begin{equation*}
   \Phi_{i}=\Gamma_{i}-mean
   \end{equation*}
   $$

4. **计算协方差矩阵及其特征值、特征向量**
   $$
   \begin{equation*}
    \begin{aligned}
    C &=\frac{1}{M} \sum_{i=1,j=1}^{M} \Phi_{i} \Phi_{j}^{T} \\
    &=A A^{T} \\
    A &=\left\{\Phi_{1}, \Phi_{2}, \Phi_{3}, \cdots \cdots, \Phi_{n}\right\}
    \end{aligned}
   \end{equation*}
   $$

   计算C的特征值${\left\{\lambda_{1}, \lambda_{2}, \ldots, \lambda_{N \times N}\right\}}$与其对应的特征向量$\left\{\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{\mathbf{N} \times \mathbf{N}}\right\}$找到前n个特征值所对应的特征向量，组成新的矩阵V，由这些特征向量组成的新图像矩阵，也就是所谓的特征脸。

5. **进行人脸识别**

   考虑一张新的人脸，缩放到相同的尺寸，然后进行特征转换，对应的公式为
   $$
   \begin{equation*}
    \begin{aligned}
   	\omega_{k}=u_{k}^{T}(\Gamma-mean)
    \end{aligned}
   \end{equation*}
   $$
   其中$k=1,2,…,M$表示对应的特征脸$u_{k}$，也就是PCA中的第$k$个特征映射矢量，通过这$M$个特征脸，可以将新的人脸转变为在特征脸的坐标系的坐标表示。   
   $$
   \begin{equation*}
   \Omega^{T}=\left[\omega_{1}, \omega_{2}, \ldots \ldots, \omega_{M}\right]
   \end{equation*}
   $$
   可以通过计算闵式距离，余弦夹角等指标判断新输入的图像和人脸库中图像的相似度。

## 求矩阵的特征值和特征向量

设$A$是$n$阶方阵，若存在数$\lambda$和非零向量$\boldsymbol{x}$使得
$$
\begin{equation*}
Ax = \lambda x
   \end{equation*}
$$
则称$\lambda$是$A$的特征值，非零向量$x$是$A$的属于特征值$\lambda$的特征向量.

> $$
> \begin{equation*}
> \begin{aligned}
>      \lambda 为A的特征值  & \Leftrightarrow Ax=\lambda x \\
>      & \Leftrightarrow \lambda x-A x=0 \\
>      & \Leftrightarrow(\lambda E-A) x=0, x \neq 0 \\
>      & \Leftrightarrow(\lambda E-A) x=0 \text { 有非零解 } \\
>      & \Leftrightarrow|\lambda E-A|=0 .  
> \end{aligned}
>    \end{equation*}
> $$

**也就是说，求解矩阵的特征值，也就是求解$|\lambda E-A|=0$的解**

**求属于$\lambda$的特征向量$\Leftrightarrow$求$(\lambda E-A)x =0$的非零解向量.**

![image-20220929101632914](%E5%A4%9A%E5%AA%92%E4%BD%93%E6%8A%80%E6%9C%AF.assets/image-20220929101632914.png)

![image-20220929101653593](%E5%A4%9A%E5%AA%92%E4%BD%93%E6%8A%80%E6%9C%AF.assets/image-20220929101653593.png)

## 如何判断两个特征向量的正交性

设两个特征向量为$a_1,a_2$
$$
\begin{equation*}
a_1=\left[ \begin{array}{c}
	x_1\\
	x_2\\
	\vdots\\
	x_n\\
\end{array} \right] ,a_2=\left[ \begin{array}{c}
	y_1\\
	y_2\\
	\vdots\\
	y_n\\
\end{array} \right] 
\end{equation*}
$$
向量正交即${\boldsymbol{a}_1}^{\boldsymbol{T}}\boldsymbol{a}_2=0$

## 什么是矩阵的秩，请举例说明

矩阵A的秩是任意矩阵A经过行变换化成阶梯矩阵后,行的首元为1的个数。

## 如何构成训练样本的特征子空间（特征脸算法的特征子空间）

**计算通过样本矩阵得到的协方差矩阵的特征值、特征向量**
$$
\begin{equation*}
 \begin{aligned}
 C &=\frac{1}{M} \sum_{i=1,j=1}^{M} \Phi_{i} \Phi_{j}^{T} \\
 &=A A^{T} \\
 A &=\left\{\Phi_{1}, \Phi_{2}, \Phi_{3}, \cdots \cdots, \Phi_{n}\right\}
 \end{aligned}
\end{equation*}
$$

计算C的特征值${\left\{\lambda_{1}, \lambda_{2}, \ldots, \lambda_{N \times N}\right\}}$与其对应的特征向量$\left\{\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{\mathbf{N} \times \mathbf{N}}\right\}$找到前n个最大特征值所对应的特征向量，组成新的矩阵V，这个矩阵V即特征子空间。

## 证明图像在经过直方均衡化后，图像的信息发生了丢失(或者说图像的信息熵减少了)

![image-20221020054754462](%E5%A4%9A%E5%AA%92%E4%BD%93%E6%8A%80%E6%9C%AF.assets/image-20221020054754462.png)

## 证明协方差矩阵的三大性质

$$
\nonumber
\left( 1 \right)\ Cov\left( Y,X \right)\ =\ Cov\left( X,Y \right) ^T\\
$$

$$
\begin{equation*}
\begin{array}{l}
	Cov\left( X,Y \right)\\
	=\frac{1}{m}\left[ \begin{matrix}
	\left( x_1-E\left( x_1 \right) \right) \left( y_1-E\left( y_1 \right) \right)&		\cdots&		\left( x_1-E\left( x_1 \right) \right) \left( y_n-E\left( y_n \right) \right)\\
	\left( x_2-E\left( x_2 \right) \right) \left( y_1-E\left( y_1 \right) \right)&		\cdots&		\left( x_2-E\left( x_2 \right) \right) \left( x_n-E\left( y_n \right) \right)\\
	\cdot&		\cdot&		\cdot\\
	\cdot&		\cdot&		\cdot\\
	\cdot&		\cdot&		\cdot\\
	\left( x_n-E\left( X_n \right) \right) \left( y_1-E\left( y_1 \right) \right)&		\cdots&		\left( x_n-E\left( x_n \right) \right) \left( y_n-E\left( y_n \right) \right)\\
\end{matrix} \right]\\
	=\frac{1}{m}\left[ \begin{array}{c}
	\left( x_1-E\left( x_1 \right) \right)\\
	\left( x_2-E\left( x_2 \right) \right)\\
	\cdot\\
	\cdot\\
	\cdot\\
	\left( x_n-E\left( x_n \right) \right)\\
\end{array} \right] \left[ y_1-E\left( y_1 \right) ,\quad y_2-E\left( y_2 \right) ,\quad ...,\quad y_n-E\left( y_n \right) \right]\\
	=E\left( (\vec{X}-E(\vec{X}))(\vec{Y}-E(\vec{Y}))^T \right)\\
	\text{故}\ Cov\left( Y,X \right) \,\,=E\left( (\vec{Y}-E(\vec{Y}))(\vec{X}-E(\vec{X}))^T \right)\\
	\,\,=Cov\left( X,Y \right) ^T\\
\end{array}
\end{equation*}
$$

$$
\nonumber
\left( 2 \right)\ Cov\left( AX+b,Y \right)\ =\ ACov\left( X,Y \right)
$$

$$
\begin{equation*}
\begin{array}{l}
	Cov\left( AX+b,Y \right) \,\,=\,\,E\left( (A\vec{X}\,\,+\,\,b-E(A\vec{X}\,\,+\,\,b))(\vec{Y}-E(\vec{Y}))^T \right)\\
	\,\,=\,\,E\left( \,\,(A\vec{X}\,\,+\,\,b\,\,-E(A\vec{X})-\,\,b\,\,)(\vec{Y}-E(\vec{Y}))^T \right)\\
	\,\,=\,\,E\left( \,\,(A\vec{X}\,\,-\,\,AE(\vec{X}))(\vec{Y}-E(\vec{Y}))^T \right) \,\,\\
	\,\,=\,\,AE\left( (\vec{X}-E(\vec{X}))(\vec{Y}-E(\vec{Y}))^T \right)\\
	\,\,=\,\,ACov\left( X,Y \right)
\end{array}
\end{equation*}
$$

$$
\nonumber
\left( 3 \right)\ Cov\left( X+Y,Z \right)\ =\ Cov\left( X,Z \right) \,\,+\,\,Cov\left( Y,Z \right)
$$

$$
\begin{equation*}
\begin{array}{l}
\,\,Cov\left( X+Y,Z \right) \,\,=\,\,E\left( ( \left( \vec{X}\,\,+\,\,\vec{Y} \right) -E\left( \vec{X}\,\,+\,\,\vec{Y} \right) \,\,)( \vec{Z}-E\left( \vec{Z} \right) \,\,)^T \right)\\
	\,\,=\,\,E\left( \left( \left( \vec{X}\,\,+\,\,\vec{Y} \right) \,\,-\,\,E\left( \vec{X} \right) \,\,-\,\,E\left( \,\,\vec{Y} \right) \right) ( \vec{Z}-E\left( \vec{Z} \right) \,\,)^T \right)\\
	\,\,=\,E\left( (\vec{X}-E(\vec{X}))(\vec{Z}-E(\vec{Z}))^T \right) \,\,\,+\,\,E\left( (\vec{Y}-E(\vec{Y}))(\vec{Z}-E(\vec{Z}))^T \right) \,\\
	\,\,=\,\,Cov\left( X,Z \right) \,\,+\,\,Cov\left( Y,Z \right) \,\,\\
\end{array}
\end{equation*}
$$

## 证明大津阈值法的类间方差公式

$$
\begin{equation*}
\sigma_{B}^{2}=w_{0}\left(\mu_{0}-\mu_{T}\right)^{2}+w_{1}\left(\mu_{1}-\mu_{T}\right)^{2}=w_{0} w_{1}\left(\mu_{1}-\mu_{0}\right)^{2}
\end{equation*}
$$

$$
\begin{equation*}
\begin{aligned}
\sigma _{B}^{2}&=w_0\left( \mu _0-\mu _T \right) ^2+w_1\left( \mu _1-\mu _T \right) ^2
\\
&=w_0\left( \mu _0-w_0\mu _0-w_1\mu _1 \right) ^2+w_1\left( \mu _1-w_0\mu _0-w_1\mu _1 \right) ^2
\\
&=w_0\left( w_1\mu _0-w_1\mu _1 \right) ^2+w_1\left( w_0\mu _1-w_0\mu _0 \right) ^2
\\
&=w_0w_1\left( \mu _1-\mu _0 \right) ^2
\end{aligned}
\end{equation*}
$$

## 傅里叶描述子基本公式的推导

我们首先假定物体的闭合边界曲线表示为一个总长度为$N$的离散的坐标序列$\{x(n),y(n)|n=0,1....N -1 \}$，则它的复数形式表示为:
$$
\begin{equation*}
	z(n)=x(n)+j y(n), n=0,1, \ldots, N-1
\end{equation*}
$$
这样这闭合曲线的边界就可以在一维空间上表示。由于闭合曲线的周期数为$N$(我们取了$N$个点来表示闭合曲线)，这一边界轮廓曲线的坐标序列也是周期的，周期长度为$N$。

一维边界轮廓曲线坐标序列的傅里叶变换如下:
$$
\nonumber
Z(k)=\sum_{n=0}^{N-1} z(n) \exp \left(-\frac{j 2 \pi k n}{N}\right),(0 \leq n \leq N-1)
$$
坐标序列的逆变换为:
$$
\nonumber
z(n)=\frac{1}{N} \sum_{k=0}^{N-1} Z(k) \exp \left(\frac{j 2 \pi k n}{N}\right),(0 \leq n \leq N-1)
$$
$Z(k)$即为傅里叶描述子

## 证明归一化傅里叶描述子的三种不变性

归一化傅里叶描述子有三种不变性，平移不变性、尺度不变性、旋转不变性。

先考虑旋转对傅里叶描述子的影响，旋转后时域表达式为
$$
z(n)^{'}\ =\ e(j\alpha)z(n)
$$
由离散傅里叶变换的线性性质，变换后的$Z(k)$为
$$
Z(k)^{'}\ =\ Z(k)e(j\alpha)
$$
接着考虑尺度变换对傅里叶描述子的影响，尺度变换后时域表达式为
$$
z(n)^{''}\ =\ \lambda z(n)
$$
由离散傅里叶变换的线性性质，变换后的$Z(k)$为
$$
Z(k)^{''}\ =\ \lambda Z(k)
$$
最后考虑平移对傅里叶描述子的影响，平移后时域表达式为
$$
z(n)^{'''}\ =\ z(n) + z_{0}
$$
由离散傅里叶变换的线性性质，变换后的$Z(k)$为
$$
Z(K)^{'''}\ =\ \sum_{n=0}^{N-1} z(n) \exp \left(-\frac{j 2 \pi k n}{N}\right) + \sum_{n=0}^{N-1} z_{0} \exp \left(-\frac{j 2 \pi k n}{N}\right)
$$
当$k$不为0时，$\displaystyle \sum_{n=0}^{N-1} z_{0} \exp \left(-\frac{j 2 \pi k n}{N}\right)$等于
$$
\begin{aligned}
\sum_{n=0}^{N-1} z_{0} \exp \left(-\frac{j 2 \pi k n}{N}\right)\ &=\ z_{0}\frac{1-exp(-\frac{j2kN\pi}{N})}{1 - exp(-\frac{j2k\pi}{N})}\\
&\ =\ z_{0}\frac{1-exp(-j2k\pi)}{1 - exp(-\frac{j2k\pi}{N})}\\
&\ =\ z_{0}\frac{1 - cos(2k\pi) - jsin(2k\pi)}{1 - exp(-\frac{j2k\pi}{N})}\\
&\ =\ z_{0}\frac{1 - 1}{1 - exp(-\frac{j2k\pi}{N})}\\
&\ =\ 0
\end{aligned}
$$
所以平移变换后傅里叶描述子只有$Z(0)$发生变换
$$
\sum_{n=0}^{N-1} z_{0} \exp \left(-\frac{j 2 \pi k n}{N}\right)\ =\ \sum_{n=0}^{N-1}z_{0} = Nz_{0}
$$

$$
Z(0)^{'''}\ =\ Z(0) + Nz_{0}
$$
归一化傅里叶描述子即去掉$Z(0)$项，将$Z(k)$除以$Z(1)$

由于去掉$Z(0)$项，显然具有尺度不变性。

由于旋转变换和尺度变换均满足
$$
\frac{Z(k)^{'}}{Z(1)^{'}}\ =\ \frac{\alpha Z(k)}{\alpha Z(1)}\ =\ \frac{Z(k)}{Z(1)}
$$
因此归一化傅里叶描述子满足尺度不变性和旋转不变性

## 若A是实矩阵，且$AA^{T}$与$A^{T}A$存在非零特征值。那么，$AA^{T}$与$A^{T}A$的非零特征值相同，且对$AA^{T}$的特征向量$v$，必然存在$A^{T}A$的一个特征向量$v^{'}$，满足关系：

$$
\nonumber
v\ =\ Av^{'}
$$

设$\alpha$是$AA^{T}$的特征值，$v$是$AA^{T}$的特征向量，那么(1)式成立
$$
AA^{T}v\ = \ \alpha v
$$
设$\beta$是$A^{T}A$的特征值，$v^{'}$是特征$A^{T}A$的特征向量，那么(2)式成立
$$
A^{T}Av^{'}\ = \ \beta v^{'}
$$
对(2)式，左右两边同时乘以A，可得
$$
AA^{T}(Av^{'})\ = \ \beta (Av^{'})
$$

故$Av^{'}$为$AA^{T}$的一个特征向量，$\beta$为$AA^{T}$的一个特征值。故得证

## 多媒体技术和人工智能的关系

一是多媒体促使人工智能向着更具可解释性的方向发展;

二是人工智能反过来为多媒体研究注入了新的思维方式，促进了多媒体技术推断能力的发展。

这两个方向形成了一个多媒体智能循环,其中多媒体和AI以交互和迭代的方式相互促进增强。

## 视觉知识的三要素

视觉知识的三要素包括**视觉概念、视觉关系和视觉推理。**

